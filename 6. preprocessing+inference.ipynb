{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Описание\n",
    "Данный ноутбук выполняет полный цикл обработки данных для иерархической классификации. \n",
    "\n",
    "Он включает:\n",
    " - предварительную очистку и преобразование текстовых и табличных признаков;\n",
    " - извлечение структурированной информации из неформализованных атрибутов;\n",
    " - генерацию текстовых эмбеддингов на основе BERT-модели;\n",
    " - предсказание категорий с помощью каскада иерархических моделей (level_1 → level_5) с учётом терминальных классов.\n",
    " \n",
    "В результате формируется финальный DataFrame с предсказанными категориями (predicted_cat) для каждого объекта."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "from catboost import Pool\n",
    "\n",
    "import argparse\n",
    "import pickle\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas(desc='Tokenizing rows')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "RAND = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(83828, 312, padding_idx=0)\n",
       "    (position_embeddings): Embedding(2048, 312)\n",
       "    (token_type_embeddings): Embedding(2, 312)\n",
       "    (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-2): 3 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=312, out_features=312, bias=True)\n",
       "            (key): Linear(in_features=312, out_features=312, bias=True)\n",
       "            (value): Linear(in_features=312, out_features=312, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=312, out_features=312, bias=True)\n",
       "            (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=312, out_features=600, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=600, out_features=312, bias=True)\n",
       "          (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=312, out_features=312, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = 'cointegrated/rubert-tiny2'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('terminators.pkl', 'rb') as f:\n",
    "    terminators = pickle.load(f)\n",
    "\n",
    "with open('clf_cat_1.pkl', 'rb') as f:\n",
    "    model_level_1 = pickle.load(f)\n",
    "\n",
    "with open('clf_cat_level_2.pkl', 'rb') as f:\n",
    "    model_level_2 = pickle.load(f)\n",
    "\n",
    "with open('clf_cat_level_3.pkl', 'rb') as f:\n",
    "    model_level_3 = pickle.load(f)\n",
    "\n",
    "with open('clf_cat_level_4.pkl', 'rb') as f:\n",
    "    model_level_4 = pickle.load(f)\n",
    "\n",
    "with open('clf_cat_level_5.pkl', 'rb') as f:\n",
    "    model_level_5 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attributes_to_text(attr_json) -> str:\n",
    "    \"\"\"\n",
    "    Извлекает атрибуты из json в строку.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        attributes = json.loads(attr_json.replace('\"\"', '\"'))\n",
    "\n",
    "        attr_texts = [\n",
    "            f\"{attr['attribute_name']}: {attr['attribute_value']}\"\n",
    "            for attr in attributes if attr['attribute_name'] not in\n",
    "            ['Название', 'В наличии', 'Ebsmstock']\n",
    "        ]\n",
    "\n",
    "        return '. '.join(attr_texts) if attr_texts else 'нет'\n",
    "\n",
    "    except Exception as e:\n",
    "        return 'Атрибуты отсутствуют'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaned_text(text) -> str:\n",
    "    \"\"\"\n",
    "    Простая очистка текста.\n",
    "    \"\"\"\n",
    "    text = str(\n",
    "        text\n",
    "    ) if text is not None else ''  # преобразуем text в строку, если это не строка\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^а-яёa-z0-9\\s.,*!?:-]', '',\n",
    "                  text)  # удаление лишних символов (кроме пунктуации)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # удаление лишних пробелов\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_info(attr_str: str) -> tuple:\n",
    "    \"\"\"\n",
    "    Извлекает структурированные данные из текстовой строки с атрибутами товара, \n",
    "    представленной в виде одного объединённого текста. Возвращает кортеж \n",
    "    с основными характеристиками товара.\n",
    "    \n",
    "    :param attr_str: cтрока с описанием атрибутов товара\n",
    "    :return: кортеж из элементов.\n",
    "    \"\"\"\n",
    "    # преобразуем attr_str в строку, если это не строка\n",
    "    attr_str = str(attr_str) if attr_str is not None else ''\n",
    "\n",
    "    # извлекаем поставщика\n",
    "    supplier_match = re.search(r'поставщик:\\s*([^\\.]+)', attr_str)\n",
    "    if supplier_match:\n",
    "        supplier = supplier_match.group(1).strip()\n",
    "        if supplier.lower() == \"нет бренда\":  # если поставщик - нет бренда\n",
    "            supplier = None\n",
    "    else:\n",
    "        supplier = None\n",
    "\n",
    "    # извлекаем модель устройства\n",
    "    model_match = re.search(r'модель устройства:\\s*([^\\.]+)', attr_str)\n",
    "    model = model_match.group(1).strip() if model_match else None\n",
    "\n",
    "    # извлекаем страну\n",
    "    country_match = re.search(r'страна:\\s*([^\\.]+)', attr_str)\n",
    "    country = country_match.group(1).strip() if country_match else None\n",
    "\n",
    "    # извлекаем материал\n",
    "    material_match = re.search(r'материал:\\s*([^\\.]+)', attr_str)\n",
    "    material = material_match.group(1).strip() if material_match else None\n",
    "\n",
    "    # извлекаем самовывоз (1 = да, 0 = нет)\n",
    "    pickup_match = re.search(r'возможность самовывоза:\\s*(да|нет)', attr_str)\n",
    "    pickup = 1 if pickup_match and pickup_match.group(1) == 'да' else 0\n",
    "\n",
    "    # извлекаем доставку (1 = да, 0 = нет)\n",
    "    delivery_match = re.search(r'возможность доставки:\\s*(да|нет)', attr_str)\n",
    "    delivery = 1 if delivery_match and delivery_match.group(1) == 'да' else 0\n",
    "\n",
    "    # извлекаем гарантию (1 = да, 0 = нет)\n",
    "    guarantee_match = re.search(r'гарантия:\\s*(да|нет)', attr_str)\n",
    "    guarantee = 1 if guarantee_match and guarantee_match.group(\n",
    "        1) == 'да' else 0\n",
    "\n",
    "    # извлекаем вес\n",
    "    weight_match = re.search(r'вес:\\s*([^\\.]+)', attr_str)\n",
    "    if weight_match:\n",
    "        weight_str = weight_match.group(1).strip()\n",
    "        # убираем запятую и пробелы, если они есть\n",
    "        weight_str = weight_str.replace(',', '.').replace(' ', '')\n",
    "        try:\n",
    "            weight = float(weight_str)\n",
    "        except ValueError:\n",
    "            weight = None  # если преобразование не удалось, устанавливаем None\n",
    "    else:\n",
    "        weight = None\n",
    "\n",
    "    # извлекаем габариты\n",
    "    size_match = re.search(\n",
    "        r'размер: длина\\s*(\\d+)\\s*ширина\\s*(\\d+)\\s*высота\\s*(\\d+)', attr_str)\n",
    "    if size_match:\n",
    "        length, width, height = map(int, size_match.groups())\n",
    "    else:\n",
    "        length, width, height = None, None, None  # если измерения не найдены\n",
    "\n",
    "    # извлекаем габариты упаковки\n",
    "    size_match_p = re.search(\n",
    "        r'ширина упаковки:\\s*(\\d+)\\.\\s*высота упаковки:\\s*(\\d+)\\.\\s*глубина упаковки:\\s*(\\d+)',\n",
    "        attr_str)\n",
    "    if size_match_p:\n",
    "        length_p, width_p, height_p = map(int, size_match_p.groups())\n",
    "    else:\n",
    "        length_p, width_p, height_p = None, None, None  # если измерения не найдены\n",
    "\n",
    "    # удаляем обработанные атрибуты из строки\n",
    "    cleaned_attrs = re.sub(r'поставщик:\\s*[^\\.]+\\.?', '', attr_str)\n",
    "    cleaned_attrs = re.sub(r'возможность самовывоза:\\s*(да|нет)\\.?', '',\n",
    "                           cleaned_attrs)\n",
    "    cleaned_attrs = re.sub(r'страна:\\s*[^\\.]+\\.?', '', cleaned_attrs)\n",
    "    cleaned_attrs = re.sub(r'материал:\\s*[^\\.]+\\.?', '', cleaned_attrs)\n",
    "    cleaned_attrs = re.sub(r'модель устройства:\\s*[^\\.]+\\.?', '',\n",
    "                           cleaned_attrs)\n",
    "    cleaned_attrs = re.sub(r'возможность доставки:\\s*(да|нет)\\.?', '',\n",
    "                           cleaned_attrs)\n",
    "    cleaned_attrs = re.sub(r'гарантия:\\s*(да|нет)\\.?', '', cleaned_attrs)\n",
    "    cleaned_attrs = re.sub(r'вес\\s*\\d+\\.?', '', cleaned_attrs)\n",
    "    cleaned_attrs = re.sub(\n",
    "        r'размер: длина\\s*\\d+\\s*ширина\\s*\\d+\\s*высота\\s*\\d+\\.?', '',\n",
    "        cleaned_attrs)\n",
    "    cleaned_attrs = re.sub(\n",
    "        r'ширина упаковки\\s*\\d+\\s*высота упаковки\\s*\\d+\\s*глубина упаковки\\s*\\d+\\.?',\n",
    "        '', cleaned_attrs)\n",
    "\n",
    "    return supplier, country, material, model, pickup, length, width, height, delivery, guarantee, weight, length_p, width_p, height_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_embedding(text: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Получает эмбеддинг текста с использованием mean pooling \n",
    "    по всем токенам (кроме паддинга).\n",
    "    \n",
    "    :param text: входной текст.\n",
    "    :return: усреднённый эмбеддинг по всем токенам.\n",
    "    \"\"\"\n",
    "    tokens = tokenizer(text,\n",
    "                       return_tensors='pt',\n",
    "                       truncation=True,\n",
    "                       padding='max_length',\n",
    "                       max_length=27)\n",
    "\n",
    "    # переносим данные на GPU, если он есть\n",
    "    tokens = {key: val.to(device) for key, val in tokens.items()}\n",
    "\n",
    "    with torch.no_grad():  # выключаем градиенты\n",
    "        output = model(**tokens)\n",
    "        last_hidden_state = output.last_hidden_state  # [batch_size, seq_len, hidden_dim]\n",
    "        attention_mask = tokens['attention_mask']  # [batch_size, seq_len]\n",
    "\n",
    "        # применяем attention mask: обнуляем эмбеддинги паддингов\n",
    "        mask = attention_mask.unsqueeze(-1).expand(last_hidden_state.size())\n",
    "        masked_embeddings = last_hidden_state * mask\n",
    "\n",
    "        # усреднение по непаддинговым токенам\n",
    "        summed = masked_embeddings.sum(dim=1)\n",
    "        counts = mask.sum(dim=1)  # число непаддинговых токенов\n",
    "        mean_pooled = summed / counts\n",
    "\n",
    "    return mean_pooled.cpu().numpy().squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Функция обрабатывает входной датасет, очищая и преобразуя текстовые и \n",
    "    категориальные признаки, извлекая информацию из текстов, \n",
    "    удаляя ненужные или малозначимые столбцы, и превращает текстовые данные \n",
    "    в эмбеддинги с помощью модели BERT. \n",
    "    Результатом является DataFrame, содержащий эмбеддинги и \n",
    "    полезные бинарные признаки.\n",
    "\n",
    "    :param dataset: исходный DataFrame.\n",
    "    :return df_embeddings: новый DataFrame, в котором каждый объект представлен \n",
    "                        эмбеддингом текста и дополнительными бинарными признаками.\n",
    "    \"\"\"\n",
    "    labeled_train = copy.copy(dataset)\n",
    "    #labeled_train.drop('hash_id', axis=1, inplace=True)\n",
    "\n",
    "    # извлекаем атрибуты из json в строку\n",
    "    labeled_train['attributes'] = labeled_train['attributes'].apply(\n",
    "        attributes_to_text)\n",
    "    # чистим текст\n",
    "    labeled_train['attributes'] = labeled_train['attributes'].apply(\n",
    "        cleaned_text)\n",
    "    labeled_train['source_name'] = labeled_train['source_name'].apply(\n",
    "        cleaned_text)\n",
    "\n",
    "    # извлекаем данные из атрибутов\n",
    "    labeled_train[[\n",
    "        'поставщик', 'страна', 'материал', 'модель', 'самовывоз', 'длина',\n",
    "        'ширина', 'высота', 'возможность доставки', 'гарантия', 'вес',\n",
    "        'длина_уп', 'ширина_уп', 'высота_уп'\n",
    "    ]] = labeled_train['attributes'].progress_apply(\n",
    "        lambda x: pd.Series(extract_info(x)))\n",
    "\n",
    "    # удаляем старый столбец\n",
    "    labeled_train.drop(columns=['attributes'], inplace=True)\n",
    "    labeled_train.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # объединяем source_name и модель в новый столбец source_name_model и удаляем их\n",
    "    labeled_train['source_name_model'] = labeled_train.apply(\n",
    "        lambda row:\n",
    "        f\"{row['source_name']}{'.' if row['модель'] is not None else ''} {row['модель'] or ''}\"\n",
    "        .strip(),\n",
    "        axis=1)\n",
    "    labeled_train.drop(['source_name', 'модель'], axis=1, inplace=True)\n",
    "\n",
    "    # удаляем остальные столбцы где много пропусков\n",
    "    labeled_train.drop([\n",
    "        'поставщик', 'страна', 'длина', 'ширина', 'высота', 'вес', 'материал',\n",
    "        'длина_уп', 'ширина_уп', 'высота_уп'\n",
    "    ],\n",
    "                       axis=1,\n",
    "                       inplace=True)\n",
    "\n",
    "    # преобразовываем данные\n",
    "    for i in labeled_train.columns:\n",
    "        if labeled_train[i].dtype == 'object':\n",
    "            labeled_train[i] = labeled_train[i].astype('category')\n",
    "        if i in ('самовывоз', 'возможность доставки', 'гарантия'):\n",
    "            labeled_train[i] = labeled_train[i].astype('int8')\n",
    "\n",
    "    # преобразуем тексты в эмбеддинги\n",
    "    texts = labeled_train['source_name_model'].tolist()\n",
    "    #labels = labeled_train['label'].values  # метки классов\n",
    "    pickup = labeled_train['самовывоз'].values\n",
    "    delivery = labeled_train['возможность доставки'].values\n",
    "    guarantee = labeled_train['гарантия'].values\n",
    "\n",
    "    # преобразуем все тексты в эмбеддинги\n",
    "    embeddings = np.array([get_bert_embedding(text) for text in tqdm(texts)])\n",
    "\n",
    "    # преобразовываем матрицу в DataFrame\n",
    "    df_embeddings = pd.DataFrame(embeddings)\n",
    "    df_embeddings['pickup'] = pickup\n",
    "    df_embeddings['delivery'] = delivery\n",
    "    df_embeddings['guarantee'] = guarantee\n",
    "\n",
    "    return df_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def levels_predict(data: pd.DataFrame, \n",
    "                   hash_data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Функция выполняет многоуровневую (иерархическую) классификацию объектов \n",
    "    с использованием последовательных моделей для каждого уровня иерархии категорий. \n",
    "    В результате функция возвращает предсказанную финальную категорию (predicted_cat) \n",
    "    для каждого объекта.\n",
    "    \n",
    "    :param data: таблица признаков объектов для инференса. \n",
    "    :param hash_data: таблица с идентификаторами hash_id, соответствующими объектам из data. \n",
    "    :return output: таблица с предсказаными категориями\n",
    "    \"\"\"\n",
    "    # level_1\n",
    "    pool = Pool(data)\n",
    "    y_pred = model_level_1.predict(pool,\n",
    "                                   prediction_type='Class',\n",
    "                                   ntree_start=0,\n",
    "                                   ntree_end=0,\n",
    "                                   thread_count=-1,\n",
    "                                   verbose=None,\n",
    "                                   task_type=\"CPU\")\n",
    "\n",
    "    data['level_1'] = y_pred\n",
    "\n",
    "    # level_2\n",
    "    pool2 = Pool(data.drop(['level_1'], axis=1))\n",
    "    y_pred2 = model_level_2.predict(pool2,\n",
    "                                    prediction_type='Class',\n",
    "                                    ntree_start=0,\n",
    "                                    ntree_end=0,\n",
    "                                    thread_count=-1,\n",
    "                                    verbose=None,\n",
    "                                    task_type='CPU')\n",
    "\n",
    "    data['level_2'] = y_pred2\n",
    "    data['hash_id'] = hash_data['hash_id']\n",
    "\n",
    "    # конечные категории для второго уровня\n",
    "    level_2_term = data[data['level_2'].isin(terminators)][[\n",
    "        'level_2', 'hash_id'\n",
    "    ]]\n",
    "    level_2_term.rename(columns={'level_2': 'predicted_cat'}, inplace=True)\n",
    "\n",
    "    # level_3\n",
    "    for_level_3 = data[(data['level_2'].isin(terminators) == False)]\n",
    "    pool3 = Pool(for_level_3.drop(['hash_id', 'level_1', 'level_2'], axis=1))\n",
    "    y_pred3 = model_level_3.predict(pool3,\n",
    "                                    prediction_type='Class',\n",
    "                                    ntree_start=0,\n",
    "                                    ntree_end=0,\n",
    "                                    thread_count=-1,\n",
    "                                    verbose=None,\n",
    "                                    task_type='CPU')\n",
    "\n",
    "    level_3_res = for_level_3\n",
    "    level_3_res['level_3'] = y_pred3\n",
    "\n",
    "    # конечные категории для третьего уровня\n",
    "    level_3_term = level_3_res[level_3_res['level_3'].isin(terminators)][[\n",
    "        'level_3', 'hash_id'\n",
    "    ]]\n",
    "    level_3_term.rename(columns={'level_3': 'predicted_cat'}, inplace=True)\n",
    "\n",
    "    # level_4\n",
    "    for_level_4 = level_3_res[(\n",
    "        level_3_res['level_3'].isin(terminators) == False)]\n",
    "    pool4 = Pool(\n",
    "        for_level_4.drop(['hash_id', 'level_1', 'level_2', 'level_3'], axis=1))\n",
    "    y_pred4 = model_level_4.predict(pool4,\n",
    "                                    prediction_type='Class',\n",
    "                                    ntree_start=0,\n",
    "                                    ntree_end=0,\n",
    "                                    thread_count=-1,\n",
    "                                    verbose=None,\n",
    "                                    task_type='CPU')\n",
    "\n",
    "    level_4_res = for_level_4\n",
    "    level_4_res['level_4'] = y_pred4\n",
    "\n",
    "    # конечные категории для четвертого уровня\n",
    "    level_4_term = level_4_res[level_4_res['level_4'].isin(terminators)][[\n",
    "        'level_4', 'hash_id'\n",
    "    ]]\n",
    "    level_4_term.rename(columns={'level_4': 'predicted_cat'}, inplace=True)\n",
    "\n",
    "    #level_5\n",
    "    for_level_5 = level_4_res[(\n",
    "        level_4_res['level_4'].isin(terminators) == False)]\n",
    "    pool5 = Pool(\n",
    "        for_level_5.drop(\n",
    "            ['hash_id', 'level_1', 'level_2', 'level_3', 'level_4'], axis=1))\n",
    "    y_pred5 = model_level_5.predict(pool5,\n",
    "                                    prediction_type='Class',\n",
    "                                    ntree_start=0,\n",
    "                                    ntree_end=0,\n",
    "                                    thread_count=-1,\n",
    "                                    verbose=None,\n",
    "                                    task_type='CPU')\n",
    "\n",
    "    level_5_res = for_level_5\n",
    "    level_5_res['level_5'] = y_pred5\n",
    "\n",
    "    # конечные категории для пятого уровня\n",
    "    level_5_term = level_5_res[level_5_res['level_5'].isin(terminators)][[\n",
    "        'level_5', 'hash_id'\n",
    "    ]]\n",
    "    level_5_term.rename(columns={'level_5': 'predicted_cat'}, inplace=True)\n",
    "\n",
    "    # объединяем предсказания по уровням в один датасет\n",
    "    level_23 = pd.concat([level_2_term, level_3_term])\n",
    "    level_234 = pd.concat([level_23, level_4_term])\n",
    "    full_level = pd.concat([level_234, level_5_term])\n",
    "\n",
    "    # сортируем значения\n",
    "    full_level.sort_index(axis=0, inplace=True)\n",
    "\n",
    "    output = pd.DataFrame()\n",
    "    output['hash_id'] = full_level['hash_id']\n",
    "    output['predicted_cat'] = full_level['predicted_cat'].astype(int)\n",
    "\n",
    "    return output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
